seed: -1
vision: True
clip_actions: 1.0
clip_observations: 5.0

policy: # only works for MlpPolicy right now
  pi_hid_sizes: [1024, 1024, 512, 512]
  vf_hid_sizes: [1024, 1024, 512, 512]
  activation: elu # can be elu, relu, selu, crelu, lrelu, tanh, sigmoid

learn:
  resume: 0
  test: False
  print_log: False
  save_interval: 5000 # check for potential saves every this many iterations
  agent_name: shadow_hand

  # rollout params
  max_iterations: 2000

  # training params
  cliprange: 0.2
  ent_coef: 0
  buffer_size: 2000
  nsteps: 1
  noptepochs: 5
  nminibatches: 4 # this is per agent
  max_grad_norm: 1
  optim_stepsize: 3.e-4 # 3e-4 is default for single agent training with constant schedule
  schedule: adaptive # could be adaptive or linear or fixed
  init_noise_std: 0.8
  desired_kl: 0.016
  gamma: 0.96
  lam: 0.95

  log_interval: 1
  asymmetric: False

  value_loss:
    apply: True
    use_clipped_value_loss: True
    value_loss_coef: 1.0
    gamma: 0.96
    lam: 0.95
    clip_range: 0.2

  expert: [
    {
      name: '0',
      path: 'example_model/state_based_model.pt',
      object_code_dict: {
      'sem/Car-669043a8ce40d9d78781f76a6db4ab62':[0.06],
      }
    },
    {
      name: '1',
      path: 'example_model/state_based_model.pt',
      object_code_dict: {
      'sem/Car-669043a8ce40d9d78781f76a6db4ab62':[0.06],
      }
    },
    {
      name: '2',
      path: 'example_model/state_based_model.pt',
      object_code_dict: {
      'sem/Car-669043a8ce40d9d78781f76a6db4ab62':[0.06],
      }
    },
  ]
